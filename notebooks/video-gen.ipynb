{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Video Generation Pipeline\n",
                "\n",
                "This notebook generates a video from a given topic or text using Ollama for scripting, Stable Diffusion (Diffusers) for images, Edge-TTS for audio, and MoviePy for assembly."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 24,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "2.5.1+cu121\n",
                        "12.1\n",
                        "True\n"
                    ]
                }
            ],
            "source": [
                "import torch\n",
                "print(torch.__version__)\n",
                "print(torch.version.cuda)\n",
                "print(torch.cuda.is_available())\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 25,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Configuration\n",
                "OLLAMA_API_URL = \"http://localhost:11434/api/generate\"\n",
                "# Use 'llama3' or another model you have installed\n",
                "OLLAMA_MODEL = \"phi3:mini\"\n",
                "\n",
                "OUTPUT_DIR = \"output\"\n",
                "SCENE_DIR = os.path.join(OUTPUT_DIR, \"scenes\")\n",
                "AUDIO_DIR = os.path.join(OUTPUT_DIR, \"audio\")\n",
                "FINAL_VIDEO_DIR = os.path.join(OUTPUT_DIR, \"video\")\n",
                "SCRIPTS_DIR = os.path.join(OUTPUT_DIR, \"scripts\")\n",
                "\n",
                "os.makedirs(SCENE_DIR, exist_ok=True)\n",
                "os.makedirs(AUDIO_DIR, exist_ok=True)\n",
                "os.makedirs(FINAL_VIDEO_DIR, exist_ok=True)\n",
                "os.makedirs(SCRIPTS_DIR, exist_ok=True)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 0: Initialize Stable Diffusion Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 26,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Loading Stable Diffusion on cuda...\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Loading weights: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 196/196 [00:00<00:00, 313.06it/s, Materializing param=text_model.final_layer_norm.weight]\n",
                        "\u001b[1mCLIPTextModel LOAD REPORT\u001b[0m from: C:\\Users\\navee\\.cache\\huggingface\\hub\\models--runwayml--stable-diffusion-v1-5\\snapshots\\451f4fe16113bff5a5d2269ed5ad43b0592e9a14\\text_encoder\n",
                        "Key                                | Status     |  | \n",
                        "-----------------------------------+------------+--+-\n",
                        "text_model.embeddings.position_ids | UNEXPECTED |  | \n",
                        "\n",
                        "\u001b[3mNotes:\n",
                        "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\u001b[0m\n",
                        "Loading pipeline components...: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [00:08<00:00,  1.36s/it]\n",
                        "You have disabled the safety checker for <class 'diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion.StableDiffusionPipeline'> by passing `safety_checker=None`. Ensure that you abide to the conditions of the Stable Diffusion license and do not expose unfiltered results in services or applications open to the public. Both the diffusers team and Hugging Face strongly recommend to keep the safety filter enabled in all public facing circumstances, disabling it only for use-cases that involve analyzing network behavior or auditing its results. For more information, please have a look at https://github.com/huggingface/diffusers/pull/254 .\n",
                        "No LoRA keys associated to CLIPTextModel found with the prefix='text_encoder'. This is safe to ignore if LoRA state dict didn't originally have any CLIPTextModel related params. You can also try specifying `prefix=None` to resolve the warning. Otherwise, open an issue if you think it's unexpected: https://github.com/huggingface/diffusers/issues/new\n"
                    ]
                }
            ],
            "source": [
                "# ===== SETTINGS =====\n",
                "model_id = \"runwayml/stable-diffusion-v1-5\"\n",
                "lora_id = \"latent-consistency/lcm-lora-sdv1-5\"\n",
                "\n",
                "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
                "\n",
                "print(f\"Loading Stable Diffusion on {device}...\")\n",
                "\n",
                "# ===== LOAD PIPELINE =====\n",
                "pipe = StableDiffusionPipeline.from_pretrained(\n",
                "    model_id,\n",
                "    torch_dtype=torch.float16 if device == \"cuda\" else torch.float32,\n",
                "    safety_checker=None\n",
                ").to(device)\n",
                "\n",
                "# Enable memory optimizations\n",
                "pipe.enable_attention_slicing()\n",
                "pipe.enable_vae_slicing()\n",
                "if device == \"cuda\":\n",
                "    pipe.enable_model_cpu_offload()\n",
                "\n",
                "# ===== LOAD LCM LoRA =====\n",
                "pipe.load_lora_weights(lora_id)\n",
                "pipe.scheduler = LCMScheduler.from_config(pipe.scheduler.config)\n",
                "\n",
                "def generate_image_sd(prompt_text, output_path):\n",
                "    print(f\"Generating image for: {prompt_text}...\")\n",
                "    prompt = f\"\"\"\n",
                "    simple flat illustration of {prompt_text},\n",
                "    minimal design,\n",
                "    clean white background,\n",
                "    educational graphic,\n",
                "    vector style,\n",
                "    no text\n",
                "    \"\"\"\n",
                "    \n",
                "    try:\n",
                "        image = pipe(\n",
                "            prompt=prompt,\n",
                "            num_inference_steps=6,      # VERY LOW = FAST\n",
                "            guidance_scale=1.5,         # LCM works best low\n",
                "            height=512,\n",
                "            width=512\n",
                "        ).images[0]\n",
                "        \n",
                "        image.save(output_path)\n",
                "        return output_path\n",
                "    except Exception as e:\n",
                "        print(f\"Error generating image: {e}\")\n",
                "        return None"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 1: Script Generation with Ollama"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 27,
            "metadata": {},
            "outputs": [],
            "source": [
                "def generate_script(topic):\n",
                "    \"\"\"\n",
                "    Generate a fresh video script from Ollama based on the given topic.\n",
                "    Always creates NEW content - never uses cached/old scripts.\n",
                "    \"\"\"\n",
                "    prompt = f\"\"\"\n",
                "    Convert this topic into a structured video plan.\n",
                "    Topic: {topic}\n",
                "    Return JSON only:\n",
                "    {{\n",
                "      \"scenes\": [\n",
                "        {{\n",
                "          \"title\": \"\",\n",
                "          \"bullets\": [],\n",
                "          \"narration\": \"\",\n",
                "          \"image_prompt\": \"visual description for illustration\"\n",
                "        }}\n",
                "      ]\n",
                "    }}\n",
                "    \"\"\"\n",
                "    \n",
                "    print(f\"üé¨ Generating NEW script for: {topic}...\")\n",
                "    print(\"‚öôÔ∏è  Connecting to Ollama...\")\n",
                "    \n",
                "    # Generate fresh content from Ollama (REQUIRED)\n",
                "    try:\n",
                "        response = requests.post(OLLAMA_API_URL, json={\n",
                "            \"model\": OLLAMA_MODEL,\n",
                "            \"prompt\": prompt,\n",
                "            \"format\": \"json\",\n",
                "            \"stream\": False\n",
                "        }, timeout=300)\n",
                "        response.raise_for_status()\n",
                "        script_data = json.loads(response.json()['response'])\n",
                "        \n",
                "        # Create a safe filename from the topic\n",
                "        safe_topic = \"\".join(c if c.isalnum() or c in (' ', '-', '_') else '_' for c in topic)\n",
                "        safe_topic = safe_topic.replace(' ', '_')[:50]  # Limit length\n",
                "        \n",
                "        # Save the fresh plan to scripts directory with topic-based name\n",
                "        timestamp = __import__('datetime').datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
                "        plan_filename = f\"{safe_topic}_{timestamp}.json\"\n",
                "        plan_file = os.path.join(SCRIPTS_DIR, plan_filename)\n",
                "        \n",
                "        with open(plan_file, \"w\") as f:\n",
                "            json.dump(script_data, f, indent=2)\n",
                "        \n",
                "        print(f\"‚úÖ Fresh script generated and saved to: {plan_file}\")\n",
                "        return script_data\n",
                "        \n",
                "    except requests.exceptions.RequestException as e:\n",
                "        print(f\"‚ùå Cannot connect to Ollama: {e}\")\n",
                "        print(\"üìã Please ensure Ollama is running:\")\n",
                "        print(\"   1. Open terminal and run: ollama serve\")\n",
                "        print(\"   2. Or ensure Ollama service is active\")\n",
                "        return None\n",
                "        \n",
                "    except json.JSONDecodeError as e:\n",
                "        print(f\"‚ùå Error decoding JSON from Ollama: {e}\")\n",
                "        print(\"   The model may have returned invalid JSON format\")\n",
                "        return None"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 2: Slide Generation (PIL)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 28,
            "metadata": {},
            "outputs": [],
            "source": [
                "def create_slide(scene, index, image_path=None):\n",
                "    width, height = 1280, 720\n",
                "    img = Image.new('RGB', (width, height), color='white')\n",
                "    draw = ImageDraw.Draw(img)\n",
                "    \n",
                "    # Fonts\n",
                "    try:\n",
                "        title_font = ImageFont.truetype(\"arial.ttf\", 60)\n",
                "        text_font = ImageFont.truetype(\"arial.ttf\", 35)\n",
                "    except:\n",
                "        title_font = ImageFont.load_default()\n",
                "        text_font = ImageFont.load_default()\n",
                "    \n",
                "    # Layout Configuration\n",
                "    margin = 50\n",
                "    content_width = width - (2 * margin)\n",
                "    \n",
                "    # If image exists, we use split layout: Text (Left) | Image (Right)\n",
                "    if image_path and os.path.exists(image_path):\n",
                "        try:\n",
                "            sd_img = Image.open(image_path)\n",
                "            # Resize to fit right side but keep aspect ratio or simple fit\n",
                "            # Let's make it 512x512 centered on the right half, or scaled nicely\n",
                "            # Right half starts at x = 640\n",
                "            \n",
                "            # Target height 600, maintain aspect\n",
                "            target_ih = 600\n",
                "            aspect = sd_img.width / sd_img.height\n",
                "            target_iw = int(target_ih * aspect)\n",
                "            \n",
                "            sd_img = sd_img.resize((target_iw, target_ih), Image.Resampling.LANCZOS)\n",
                "            \n",
                "            # Position on right side\n",
                "            img_x = 640 + (640 - target_iw) // 2\n",
                "            img_y = (720 - target_ih) // 2\n",
                "            \n",
                "            img.paste(sd_img, (img_x, img_y))\n",
                "            \n",
                "            # Constrain text to left half\n",
                "            content_width = 580 # 640 - margin - padding\n",
                "        except Exception as e:\n",
                "            print(f\"Error placing image: {e}\")\n",
                "\n",
                "    # Draw Title\n",
                "    title_text = scene.get('title', f\"Scene {index}\")\n",
                "    # Wrap title if needed\n",
                "    title_lines = textwrap.wrap(title_text, width=20 if content_width < 600 else 40)\n",
                "    ty = 50\n",
                "    for line in title_lines:\n",
                "        draw.text((margin, ty), line, fill='black', font=title_font)\n",
                "        ty += 70\n",
                "    \n",
                "    # Draw Bullets\n",
                "    y = ty + 30\n",
                "    bullets = scene.get('bullets', [])\n",
                "    for bullet in bullets:\n",
                "        lines = textwrap.wrap(bullet, width=30 if content_width < 600 else 50)\n",
                "        for line in lines:\n",
                "            draw.text((margin + 30, y), f\"‚Ä¢ {line}\", fill='black', font=text_font)\n",
                "            y += 45\n",
                "            \n",
                "    filename = os.path.join(SCENE_DIR, f\"scene_{index}.png\")\n",
                "    img.save(filename)\n",
                "    return filename"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 3: Audio Generation (Edge-TTS)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 29,
            "metadata": {},
            "outputs": [],
            "source": [
                "async def generate_audio(text, index):\n",
                "    voice = \"en-US-ChristopherNeural\"\n",
                "    output_file = os.path.join(AUDIO_DIR, f\"scene_{index}.mp3\")\n",
                "    \n",
                "    print(f\"Generating audio for scene {index}...\")\n",
                "    try:\n",
                "        communicate = edge_tts.Communicate(text, voice)\n",
                "        await communicate.save(output_file)\n",
                "        return output_file\n",
                "    except Exception as e:\n",
                "        print(f\"Error generating audio: {e}\")\n",
                "        return None"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 4: Video Assembly (MoviePy)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 30,
            "metadata": {},
            "outputs": [],
            "source": [
                "def create_video_clip(image_path, audio_path, index):\n",
                "    output_path = os.path.join(FINAL_VIDEO_DIR, f\"scene_{index}.mp4\")\n",
                "    \n",
                "    print(f\"Creating video clip for scene {index} using MoviePy...\")\n",
                "    try:\n",
                "        # Load audio first to get duration\n",
                "        audio_clip = AudioFileClip(audio_path)\n",
                "        \n",
                "        # Create video clip with proper FPS setting before adding audio\n",
                "        video_clip = (ImageClip(image_path)\n",
                "                     .with_duration(audio_clip.duration)\n",
                "                     .with_fps(24)  # Must set FPS before adding audio\n",
                "                     .with_audio(audio_clip))\n",
                "        \n",
                "        # Write with explicit audio settings for better compatibility\n",
                "        video_clip.write_videofile(\n",
                "            output_path, \n",
                "            fps=24, \n",
                "            codec='libx264',\n",
                "            audio_codec='aac',\n",
                "            audio_bitrate='192k',\n",
                "            preset='medium',\n",
                "            threads=4,\n",
                "            logger=None  # Suppress verbose output\n",
                "        )\n",
                "        \n",
                "        # Close clips to ensure file writes are complete\n",
                "        video_clip.close()\n",
                "        audio_clip.close()\n",
                "        \n",
                "        # Small delay to ensure file is fully written\n",
                "        import time\n",
                "        time.sleep(0.5)\n",
                "        \n",
                "        # Verify the output file has audio\n",
                "        test_clip = VideoFileClip(output_path)\n",
                "        has_audio = test_clip.audio is not None\n",
                "        test_clip.close()\n",
                "        \n",
                "        if has_audio:\n",
                "            print(f\"‚úì Audio verified in scene {index}\")\n",
                "        else:\n",
                "            print(f\"‚ö†Ô∏è  Warning: Audio missing from scene {index}\")\n",
                "        \n",
                "        return output_path\n",
                "    except Exception as e:\n",
                "        print(f\"MoviePy failed for scene {index}: {e}\")\n",
                "        import traceback\n",
                "        traceback.print_exc()\n",
                "        return None"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 5: Merge All Scenes"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 31,
            "metadata": {},
            "outputs": [],
            "source": [
                "def merge_scenes(video_files):\n",
                "    output_filename = os.path.join(FINAL_VIDEO_DIR, \"final_video.mp4\")\n",
                "    \n",
                "    print(\"Merging all scenes into final video...\")\n",
                "    try:\n",
                "        clips = []\n",
                "        for f in video_files:\n",
                "            print(f\"Loading {os.path.basename(f)}...\")\n",
                "            clip = VideoFileClip(f)\n",
                "            \n",
                "            # Verify audio is present\n",
                "            if clip.audio is None:\n",
                "                print(f\"‚ö†Ô∏è  Warning: No audio in {f}\")\n",
                "            else:\n",
                "                print(f\"‚úì Audio loaded: {clip.audio.duration:.2f}s\")\n",
                "            \n",
                "            clips.append(clip)\n",
                "        \n",
                "        print(f\"\\nConcatenating {len(clips)} clips...\")\n",
                "        # Use default method (chain) which preserves audio better\n",
                "        final_clip = concatenate_videoclips(clips)\n",
                "        \n",
                "        print(\"Writing final video with audio...\")\n",
                "        final_clip.write_videofile(\n",
                "            output_filename, \n",
                "            fps=24, \n",
                "            codec='libx264',\n",
                "            audio_codec='aac',\n",
                "            audio_bitrate='192k',\n",
                "            preset='medium',\n",
                "            threads=4,\n",
                "            logger=None  # Suppress verbose output\n",
                "        )\n",
                "        \n",
                "        # Close all clips to free resources\n",
                "        final_clip.close()\n",
                "        for clip in clips:\n",
                "            clip.close()\n",
                "        \n",
                "        print(f\"‚úÖ Done! Output: {output_filename}\")\n",
                "        return output_filename\n",
                "    except Exception as e:\n",
                "        print(f\"‚ùå Error merging scenes: {e}\")\n",
                "        import traceback\n",
                "        traceback.print_exc()\n",
                "        return None"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Execution Pipeline"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 32,
            "metadata": {},
            "outputs": [],
            "source": [
                "async def main(topic):\n",
                "    \"\"\"\n",
                "    Main video generation pipeline.\n",
                "    Always generates fresh content based on the given topic.\n",
                "    \"\"\"\n",
                "    # 1. Generate Script (FRESH - never uses cache)\n",
                "    script_data = generate_script(topic)\n",
                "    if not script_data:\n",
                "        print(\"‚ùå Cannot proceed without a valid script. Exiting...\")\n",
                "        return\n",
                "    \n",
                "    scenes = script_data.get('scenes', [])\n",
                "    if not scenes:\n",
                "        print(\"‚ùå No scenes found in script. Exiting...\")\n",
                "        return\n",
                "    \n",
                "    video_clips = []\n",
                "    \n",
                "    print(f\"\\nüé• Starting video generation for {len(scenes)} scenes...\\n\")\n",
                "    \n",
                "    for i, scene in enumerate(scenes, 1):\n",
                "        title = scene.get('title')\n",
                "        print(f\"üìç Processing Scene {i}/{len(scenes)}: {title}\")\n",
                "        \n",
                "        # 1.5 Generate Image (SD)\n",
                "        image_prompt = scene.get('image_prompt')\n",
                "        generated_img_path = None\n",
                "        if image_prompt:\n",
                "             # Create a safe filename for the raw SD generation\n",
                "             raw_img_path = os.path.join(SCENE_DIR, f\"scene_{i}_raw.png\")\n",
                "             generated_img_path = generate_image_sd(image_prompt, raw_img_path)\n",
                "        \n",
                "        if not generated_img_path:\n",
                "             # Fallback if generation failed or no prompt, create_slide handles None\n",
                "             print(\"‚ö†Ô∏è  No image generated, using text-only layout.\")\n",
                "\n",
                "        # 2. Create Slide with Text + Image\n",
                "        img_path = create_slide(scene, i, image_path=generated_img_path)\n",
                "        \n",
                "        # 3. Narration -> Audio\n",
                "        narration = scene.get('narration', '')\n",
                "        if not narration:\n",
                "            print(f\"‚ö†Ô∏è  Warning: No narration for scene {i}, skipping...\")\n",
                "            continue\n",
                "            \n",
                "        audio_path = await generate_audio(narration, i)\n",
                "        if not audio_path:\n",
                "            continue\n",
                "            \n",
                "        # 4. Combine -> Video Clip\n",
                "        clip_path = create_video_clip(img_path, audio_path, i)\n",
                "        if clip_path:\n",
                "            video_clips.append(clip_path)\n",
                "            print(f\"‚úÖ Scene {i} completed\\n\")\n",
                "            \n",
                "    # 5. Merge All Scenes\n",
                "    if video_clips:\n",
                "        print(f\"\\nüé¨ Merging {len(video_clips)} scenes into final video...\")\n",
                "        final_path = merge_scenes(video_clips)\n",
                "        if final_path:\n",
                "            print(f\"\\nüéâ SUCCESS! Your video is ready: {final_path}\")\n",
                "    else:\n",
                "        print(\"‚ùå No video clips were created. Cannot generate final video.\")\n",
                "\n",
                "# Example Usage"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 33,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "üé¨ Generating NEW script for: Model context protocol...\n",
                        "‚öôÔ∏è  Connecting to Ollama...\n",
                        "‚úÖ Fresh script generated and saved to: output\\scripts\\Model_context_protocol_20260215_211021.json\n",
                        "\n",
                        "üé• Starting video generation for 3 scenes...\n",
                        "\n",
                        "üìç Processing Scene 1/3: Understanding Model Context Protocol\n",
                        "Generating image for: A flowchart illustrating the key components involved when deploying a ML model using MCP...\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [00:02<00:00,  2.21it/s]\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Generating audio for scene 1...\n",
                        "Creating video clip for scene 1 using MoviePy...\n",
                        "‚úì Audio verified in scene 1\n",
                        "‚úÖ Scene 1 completed\n",
                        "\n",
                        "üìç Processing Scene 2/3: Input, Output and Environment Interaction\n",
                        "Generating image for: Side-by-side visual representation of input data capture process versus output interpretation in MCP...\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [00:02<00:00,  2.25it/s]\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Generating audio for scene 2...\n",
                        "Creating video clip for scene 2 using MoviePy...\n",
                        "‚úì Audio verified in scene 2\n",
                        "‚úÖ Scene 2 completed\n",
                        "\n",
                        "üìç Processing Scene 3/3: Handling Environment Interaction\n",
                        "Generating image for: Visual depiction of environment-model interaction using MCP framework...\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [00:02<00:00,  2.35it/s]\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Generating audio for scene 3...\n",
                        "Creating video clip for scene 3 using MoviePy...\n",
                        "‚úì Audio verified in scene 3\n",
                        "‚úÖ Scene 3 completed\n",
                        "\n",
                        "\n",
                        "üé¨ Merging 3 scenes into final video...\n",
                        "Merging all scenes into final video...\n",
                        "Loading scene_1.mp4...\n",
                        "‚úì Audio loaded: 15.10s\n",
                        "Loading scene_2.mp4...\n",
                        "‚úì Audio loaded: 12.43s\n",
                        "Loading scene_3.mp4...\n",
                        "‚úì Audio loaded: 13.51s\n",
                        "\n",
                        "Concatenating 3 clips...\n",
                        "Writing final video with audio...\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "c:\\Users\\navee\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\moviepy\\video\\io\\ffmpeg_reader.py:190: UserWarning: In file output\\video\\scene_1.mp4, 2764800 bytes wanted but 0 bytes read at frame index 362 (out of a total 362 frames), at time 15.08/15.10 sec. Using the last valid frame instead.\n",
                        "  warnings.warn(\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "‚úÖ Done! Output: output\\video\\final_video.mp4\n",
                        "\n",
                        "üéâ SUCCESS! Your video is ready: output\\video\\final_video.mp4\n"
                    ]
                }
            ],
            "source": [
                "await main(\"Model context protocol\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.9"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
