{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Video Generation Pipeline\n",
                "\n",
                "This notebook generates a video from a given topic or text using Ollama for scripting, Stable Diffusion (Diffusers) for images, Edge-TTS for audio, and MoviePy for assembly."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [],
            "source": [
                "import json\n",
                "import os\n",
                "import textwrap\n",
                "import requests\n",
                "import asyncio\n",
                "from PIL import Image, ImageDraw, ImageFont\n",
                "import edge_tts\n",
                "from moviepy import ImageClip, AudioFileClip, concatenate_videoclips, VideoFileClip\n",
                "import torch\n",
                "from diffusers import StableDiffusionPipeline, LCMScheduler\n",
                "from peft import get_peft_model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "2.5.1+cu121\n",
                        "12.1\n",
                        "True\n"
                    ]
                }
            ],
            "source": [
                "import torch\n",
                "print(torch.__version__)\n",
                "print(torch.version.cuda)\n",
                "print(torch.cuda.is_available())\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Configuration\n",
                "OLLAMA_API_URL = \"http://localhost:11434/api/generate\"\n",
                "# Use 'llama3' or another model you have installed\n",
                "OLLAMA_MODEL = \"phi3:mini\"\n",
                "\n",
                "OUTPUT_DIR = \"output\"\n",
                "SCENE_DIR = os.path.join(OUTPUT_DIR, \"scenes\")\n",
                "AUDIO_DIR = os.path.join(OUTPUT_DIR, \"audio\")\n",
                "FINAL_VIDEO_DIR = os.path.join(OUTPUT_DIR, \"video\")\n",
                "\n",
                "os.makedirs(SCENE_DIR, exist_ok=True)\n",
                "os.makedirs(AUDIO_DIR, exist_ok=True)\n",
                "os.makedirs(FINAL_VIDEO_DIR, exist_ok=True)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 0: Initialize Stable Diffusion Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Loading Stable Diffusion on cuda...\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Loading weights: 100%|██████████| 196/196 [00:00<00:00, 377.02it/s, Materializing param=text_model.final_layer_norm.weight]\n",
                        "\u001b[1mCLIPTextModel LOAD REPORT\u001b[0m from: C:\\Users\\navee\\.cache\\huggingface\\hub\\models--runwayml--stable-diffusion-v1-5\\snapshots\\451f4fe16113bff5a5d2269ed5ad43b0592e9a14\\text_encoder\n",
                        "Key                                | Status     |  | \n",
                        "-----------------------------------+------------+--+-\n",
                        "text_model.embeddings.position_ids | UNEXPECTED |  | \n",
                        "\n",
                        "\u001b[3mNotes:\n",
                        "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\u001b[0m\n",
                        "Loading pipeline components...: 100%|██████████| 6/6 [00:04<00:00,  1.48it/s]\n",
                        "You have disabled the safety checker for <class 'diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion.StableDiffusionPipeline'> by passing `safety_checker=None`. Ensure that you abide to the conditions of the Stable Diffusion license and do not expose unfiltered results in services or applications open to the public. Both the diffusers team and Hugging Face strongly recommend to keep the safety filter enabled in all public facing circumstances, disabling it only for use-cases that involve analyzing network behavior or auditing its results. For more information, please have a look at https://github.com/huggingface/diffusers/pull/254 .\n",
                        "No LoRA keys associated to CLIPTextModel found with the prefix='text_encoder'. This is safe to ignore if LoRA state dict didn't originally have any CLIPTextModel related params. You can also try specifying `prefix=None` to resolve the warning. Otherwise, open an issue if you think it's unexpected: https://github.com/huggingface/diffusers/issues/new\n"
                    ]
                }
            ],
            "source": [
                "# ===== SETTINGS =====\n",
                "model_id = \"runwayml/stable-diffusion-v1-5\"\n",
                "lora_id = \"latent-consistency/lcm-lora-sdv1-5\"\n",
                "\n",
                "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
                "\n",
                "print(f\"Loading Stable Diffusion on {device}...\")\n",
                "\n",
                "# ===== LOAD PIPELINE =====\n",
                "pipe = StableDiffusionPipeline.from_pretrained(\n",
                "    model_id,\n",
                "    torch_dtype=torch.float16 if device == \"cuda\" else torch.float32,\n",
                "    safety_checker=None\n",
                ").to(device)\n",
                "\n",
                "# Enable memory optimizations\n",
                "pipe.enable_attention_slicing()\n",
                "pipe.enable_vae_slicing()\n",
                "if device == \"cuda\":\n",
                "    pipe.enable_model_cpu_offload()\n",
                "\n",
                "# ===== LOAD LCM LoRA =====\n",
                "pipe.load_lora_weights(lora_id)\n",
                "pipe.scheduler = LCMScheduler.from_config(pipe.scheduler.config)\n",
                "\n",
                "def generate_image_sd(prompt_text, output_path):\n",
                "    print(f\"Generating image for: {prompt_text}...\")\n",
                "    prompt = f\"\"\"\n",
                "    simple flat illustration of {prompt_text},\n",
                "    minimal design,\n",
                "    clean white background,\n",
                "    educational graphic,\n",
                "    vector style,\n",
                "    no text\n",
                "    \"\"\"\n",
                "    \n",
                "    try:\n",
                "        image = pipe(\n",
                "            prompt=prompt,\n",
                "            num_inference_steps=6,      # VERY LOW = FAST\n",
                "            guidance_scale=1.5,         # LCM works best low\n",
                "            height=512,\n",
                "            width=512\n",
                "        ).images[0]\n",
                "        \n",
                "        image.save(output_path)\n",
                "        return output_path\n",
                "    except Exception as e:\n",
                "        print(f\"Error generating image: {e}\")\n",
                "        return None"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 1: Script Generation with Ollama"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {},
            "outputs": [],
            "source": [
                "def generate_script(topic):\n",
                "    prompt = f\"\"\"\n",
                "    Convert this topic into a structured video plan.\n",
                "    Topic: {topic}\n",
                "    Return JSON only:\n",
                "    {{\n",
                "      \"scenes\": [\n",
                "        {{\n",
                "          \"title\": \"\",\n",
                "          \"bullets\": [],\n",
                "          \"narration\": \"\",\n",
                "          \"image_prompt\": \"visual description for illustration\"\n",
                "        }}\n",
                "      ]\n",
                "    }}\n",
                "    \"\"\"\n",
                "    \n",
                "    print(f\"Generating script for: {topic}...\")\n",
                "    try:\n",
                "        response = requests.post(OLLAMA_API_URL, json={\n",
                "            \"model\": OLLAMA_MODEL,\n",
                "            \"prompt\": prompt,\n",
                "            \"format\": \"json\",\n",
                "            \"stream\": False\n",
                "        })\n",
                "        response.raise_for_status()\n",
                "        return json.loads(response.json()['response'])\n",
                "    except requests.exceptions.RequestException as e:\n",
                "        print(f\"Error connecting to Ollama: {e}\")\n",
                "        return None\n",
                "    except json.JSONDecodeError:\n",
                "        print(\"Error decoding JSON response from Ollama.\")\n",
                "        return None"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 2: Slide Generation (PIL)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "metadata": {},
            "outputs": [],
            "source": [
                "def create_slide(scene, index, image_path=None):\n",
                "    width, height = 1280, 720\n",
                "    img = Image.new('RGB', (width, height), color='white')\n",
                "    draw = ImageDraw.Draw(img)\n",
                "    \n",
                "    # Fonts\n",
                "    try:\n",
                "        title_font = ImageFont.truetype(\"arial.ttf\", 60)\n",
                "        text_font = ImageFont.truetype(\"arial.ttf\", 35)\n",
                "    except:\n",
                "        title_font = ImageFont.load_default()\n",
                "        text_font = ImageFont.load_default()\n",
                "    \n",
                "    # Layout Configuration\n",
                "    margin = 50\n",
                "    content_width = width - (2 * margin)\n",
                "    \n",
                "    # If image exists, we use split layout: Text (Left) | Image (Right)\n",
                "    if image_path and os.path.exists(image_path):\n",
                "        try:\n",
                "            sd_img = Image.open(image_path)\n",
                "            # Resize to fit right side but keep aspect ratio or simple fit\n",
                "            # Let's make it 512x512 centered on the right half, or scaled nicely\n",
                "            # Right half starts at x = 640\n",
                "            \n",
                "            # Target height 600, maintain aspect\n",
                "            target_ih = 600\n",
                "            aspect = sd_img.width / sd_img.height\n",
                "            target_iw = int(target_ih * aspect)\n",
                "            \n",
                "            sd_img = sd_img.resize((target_iw, target_ih), Image.Resampling.LANCZOS)\n",
                "            \n",
                "            # Position on right side\n",
                "            img_x = 640 + (640 - target_iw) // 2\n",
                "            img_y = (720 - target_ih) // 2\n",
                "            \n",
                "            img.paste(sd_img, (img_x, img_y))\n",
                "            \n",
                "            # Constrain text to left half\n",
                "            content_width = 580 # 640 - margin - padding\n",
                "        except Exception as e:\n",
                "            print(f\"Error placing image: {e}\")\n",
                "\n",
                "    # Draw Title\n",
                "    title_text = scene.get('title', f\"Scene {index}\")\n",
                "    # Wrap title if needed\n",
                "    title_lines = textwrap.wrap(title_text, width=20 if content_width < 600 else 40)\n",
                "    ty = 50\n",
                "    for line in title_lines:\n",
                "        draw.text((margin, ty), line, fill='black', font=title_font)\n",
                "        ty += 70\n",
                "    \n",
                "    # Draw Bullets\n",
                "    y = ty + 30\n",
                "    bullets = scene.get('bullets', [])\n",
                "    for bullet in bullets:\n",
                "        lines = textwrap.wrap(bullet, width=30 if content_width < 600 else 50)\n",
                "        for line in lines:\n",
                "            draw.text((margin + 30, y), f\"• {line}\", fill='black', font=text_font)\n",
                "            y += 45\n",
                "            \n",
                "    filename = os.path.join(SCENE_DIR, f\"scene_{index}.png\")\n",
                "    img.save(filename)\n",
                "    return filename"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 3: Audio Generation (Edge-TTS)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "metadata": {},
            "outputs": [],
            "source": [
                "async def generate_audio(text, index):\n",
                "    voice = \"en-US-ChristopherNeural\"\n",
                "    output_file = os.path.join(AUDIO_DIR, f\"scene_{index}.mp3\")\n",
                "    \n",
                "    print(f\"Generating audio for scene {index}...\")\n",
                "    try:\n",
                "        communicate = edge_tts.Communicate(text, voice)\n",
                "        await communicate.save(output_file)\n",
                "        return output_file\n",
                "    except Exception as e:\n",
                "        print(f\"Error generating audio: {e}\")\n",
                "        return None"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 4: Video Assembly (MoviePy)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 21,
            "metadata": {},
            "outputs": [],
            "source": [
                "def create_video_clip(image_path, audio_path, index):\n",
                "    output_path = os.path.join(FINAL_VIDEO_DIR, f\"scene_{index}.mp4\")\n",
                "    \n",
                "    print(f\"Creating video clip for scene {index} using MoviePy...\")\n",
                "    try:\n",
                "        audio_clip = AudioFileClip(audio_path)\n",
                "        video_clip = ImageClip(image_path).with_duration(audio_clip.duration)\n",
                "        video_clip = video_clip.with_audio(audio_clip)\n",
                "        video_clip.write_videofile(output_path, fps=24, codec='libx264', audio_codec='aac')\n",
                "        return output_path\n",
                "    except Exception as e:\n",
                "        print(f\"MoviePy failed for scene {index}: {e}\")\n",
                "        return None"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 5: Merge All Scenes"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 22,
            "metadata": {},
            "outputs": [],
            "source": [
                "def merge_scenes(video_files):\n",
                "    output_filename = \"final_video.mp4\"\n",
                "    \n",
                "    print(\"Merging all scenes into final video...\")\n",
                "    try:\n",
                "        clips = [VideoFileClip(f) for f in video_files]\n",
                "        final_clip = concatenate_videoclips(clips)\n",
                "        final_clip.write_videofile(output_filename, fps=24, codec='libx264', audio_codec='aac')\n",
                "        print(f\"Done! Output: {output_filename}\")\n",
                "        return output_filename\n",
                "    except Exception as e:\n",
                "        print(f\"Error merging scenes: {e}\")\n",
                "        return None"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Execution Pipeline"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 23,
            "metadata": {},
            "outputs": [],
            "source": [
                "async def main(topic):\n",
                "    # 1. Generate Script\n",
                "    script_data = generate_script(topic)\n",
                "    if not script_data:\n",
                "        return\n",
                "    \n",
                "    # Save plan for reference\n",
                "    with open(\"video_plan.json\", \"w\") as f:\n",
                "        json.dump(script_data, f, indent=2)\n",
                "    \n",
                "    scenes = script_data.get('scenes', [])\n",
                "    video_clips = []\n",
                "    \n",
                "    for i, scene in enumerate(scenes, 1):\n",
                "        title = scene.get('title')\n",
                "        print(f\"Processing Scene {i}: {title}\")\n",
                "        \n",
                "        # 1.5 Generate Image (SD)\n",
                "        image_prompt = scene.get('image_prompt')\n",
                "        generated_img_path = None\n",
                "        if image_prompt:\n",
                "             # Create a safe filename for the raw SD generation\n",
                "             raw_img_path = os.path.join(SCENE_DIR, f\"scene_{i}_raw.png\")\n",
                "             generated_img_path = generate_image_sd(image_prompt, raw_img_path)\n",
                "        \n",
                "        if not generated_img_path:\n",
                "             # Fallback if generation failed or no prompt, create_slide handles None\n",
                "             print(\"No image generated, using text-only layout.\")\n",
                "\n",
                "        # 2. Tools -> Image (Slide)\n",
                "        img_path = create_slide(scene, i, image_path=generated_img_path)\n",
                "        \n",
                "        # 3. Narration -> Audio\n",
                "        narration = scene.get('narration', '')\n",
                "        if not narration:\n",
                "            print(f\"Warning: No narration for scene {i}\")\n",
                "            continue\n",
                "            \n",
                "        audio_path = await generate_audio(narration, i)\n",
                "        if not audio_path:\n",
                "            continue\n",
                "            \n",
                "        # 4. Combine -> Clip\n",
                "        clip_path = create_video_clip(img_path, audio_path, i)\n",
                "        if clip_path:\n",
                "            video_clips.append(clip_path)\n",
                "            \n",
                "    # 5. Merge\n",
                "    if video_clips:\n",
                "        merge_scenes(video_clips)\n",
                "    else:\n",
                "        print(\"No video clips were created.\")\n",
                "\n",
                "# Example Usage\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 24,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Generating script for: The history of the internet...\n",
                        "Processing Scene 1: The Origins of the Internet\n",
                        "Generating image for: 1960's era computer with ARPANET equipment....\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "100%|██████████| 6/6 [00:29<00:00,  4.91s/it]\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Generating audio for scene 1...\n",
                        "Creating video clip for scene 1 using MoviePy...\n",
                        "MoviePy - Building video output\\video\\scene_1.mp4.\n",
                        "MoviePy - Writing audio in scene_1TEMP_MPY_wvf_snd.mp4\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "                                                                   \r"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "MoviePy - Done.\n",
                        "MoviePy - Writing video output\\video\\scene_1.mp4\n",
                        "\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "                                                                         \r"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "MoviePy - Done !\n",
                        "MoviePy - video ready output\\video\\scene_1.mp4\n",
                        "Processing Scene 2: The Birth of the Internet\n",
                        "Generating image for: First webpage with hyperlinks in 1990's era computer....\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "100%|██████████| 6/6 [00:29<00:00,  4.92s/it]\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Generating audio for scene 2...\n",
                        "Creating video clip for scene 2 using MoviePy...\n",
                        "MoviePy - Building video output\\video\\scene_2.mp4.\n",
                        "MoviePy - Writing audio in scene_2TEMP_MPY_wvf_snd.mp4\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "                                                                    \r"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "MoviePy - Done.\n",
                        "MoviePy - Writing video output\\video\\scene_2.mp4\n",
                        "\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "                                                                         \r"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "MoviePy - Done !\n",
                        "MoviePy - video ready output\\video\\scene_2.mp4\n",
                        "Processing Scene 3: Evolution and Expansion of the Internet\n",
                        "Generating image for: Early FTP server interface on computer with internet connection....\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "100%|██████████| 6/6 [00:29<00:00,  4.91s/it]\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Generating audio for scene 3...\n",
                        "Creating video clip for scene 3 using MoviePy...\n",
                        "MoviePy - Building video output\\video\\scene_3.mp4.\n",
                        "MoviePy - Writing audio in scene_3TEMP_MPY_wvf_snd.mp4\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "                                                                    \r"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "MoviePy - Done.\n",
                        "MoviePy - Writing video output\\video\\scene_3.mp4\n",
                        "\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "                                                                         \r"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "MoviePy - Done !\n",
                        "MoviePy - video ready output\\video\\scene_3.mp4\n",
                        "Processing Scene 4: The Internet Today\n",
                        "Generating image for: Smartphone displaying a web browser with popular social media icons....\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "100%|██████████| 6/6 [00:29<00:00,  4.92s/it]\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Generating audio for scene 4...\n",
                        "Creating video clip for scene 4 using MoviePy...\n",
                        "MoviePy - Building video output\\video\\scene_4.mp4.\n",
                        "MoviePy - Writing audio in scene_4TEMP_MPY_wvf_snd.mp4\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "                                                                    \r"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "MoviePy - Done.\n",
                        "MoviePy - Writing video output\\video\\scene_4.mp4\n",
                        "\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "                                                                         \r"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "MoviePy - Done !\n",
                        "MoviePy - video ready output\\video\\scene_4.mp4\n",
                        "Merging all scenes into final video...\n",
                        "MoviePy - Building video final_video.mp4.\n",
                        "MoviePy - Writing audio in final_videoTEMP_MPY_wvf_snd.mp4\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "                                                                      \r"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "MoviePy - Done.\n",
                        "MoviePy - Writing video final_video.mp4\n",
                        "\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "frame_index:  22%|██▏       | 342/1552 [00:01<00:04, 284.91it/s, now=None]c:\\Users\\navee\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\moviepy\\video\\io\\ffmpeg_reader.py:190: UserWarning: In file output\\video\\scene_1.mp4, 2764800 bytes wanted but 0 bytes read at frame index 351 (out of a total 351 frames), at time 14.62/14.66 sec. Using the last valid frame instead.\n",
                        "  warnings.warn(\n",
                        "frame_index:  50%|████▉     | 772/1552 [00:02<00:02, 302.14it/s, now=None]c:\\Users\\navee\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\moviepy\\video\\io\\ffmpeg_reader.py:190: UserWarning: In file output\\video\\scene_2.mp4, 2764800 bytes wanted but 0 bytes read at frame index 433 (out of a total 433 frames), at time 18.04/18.07 sec. Using the last valid frame instead.\n",
                        "  warnings.warn(\n",
                        "                                                                           "
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "MoviePy - Done !\n",
                        "MoviePy - video ready final_video.mp4\n",
                        "Done! Output: final_video.mp4\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "\r"
                    ]
                }
            ],
            "source": [
                "await main(\"The history of the internet\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.9"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
