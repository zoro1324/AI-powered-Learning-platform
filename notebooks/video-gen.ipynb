{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 15,
            "metadata": {},
            "outputs": [],
            "source": [
                "import json\n",
                "import os\n",
                "import textwrap\n",
                "import requests\n",
                "import asyncio\n",
                "from PIL import Image, ImageDraw, ImageFont\n",
                "import edge_tts\n",
                "from moviepy import ImageClip, AudioFileClip, concatenate_videoclips, VideoFileClip\n",
                "import torch\n",
                "from diffusers import StableDiffusionPipeline, LCMScheduler\n",
                "from peft import get_peft_model    "
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Video Generation Pipeline\n",
                "\n",
                "This notebook generates a video from a given topic or text using Ollama for scripting, Stable Diffusion (Diffusers) for images, Edge-TTS for audio, and MoviePy for assembly."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## ‚ö†Ô∏è Important: Package Compatibility Fix\n",
                "\n",
                "**If you encounter a `ValueError` with transformers/diffusers, run the following command to upgrade packages:**\n",
                "\n",
                "The error occurs when PyTorch < 2.4 is installed, but transformers requires >= 2.4. The requirements.txt has been updated with pinned compatible versions."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 16,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Run this command to upgrade packages to compatible versions\n",
                "# Uncomment the line below and run this cell if you encounter version errors\n",
                "# !pip install --upgrade -r ../requirements.txt"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 17,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "2.3.1+cu121\n",
                        "12.1\n",
                        "True\n"
                    ]
                }
            ],
            "source": [
                "import torch\n",
                "print(torch.__version__)\n",
                "print(torch.version.cuda)\n",
                "print(torch.cuda.is_available())\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 18,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Configuration\n",
                "OLLAMA_API_URL = \"http://localhost:11434/api/generate\"\n",
                "# Use 'llama3' or another model you have installed\n",
                "OLLAMA_MODEL = \"phi3:mini\"\n",
                "\n",
                "OUTPUT_DIR = \"output\"\n",
                "SCENE_DIR = os.path.join(OUTPUT_DIR, \"scenes\")\n",
                "AUDIO_DIR = os.path.join(OUTPUT_DIR, \"audio\")\n",
                "FINAL_VIDEO_DIR = os.path.join(OUTPUT_DIR, \"video\")\n",
                "SCRIPTS_DIR = os.path.join(OUTPUT_DIR, \"scripts\")\n",
                "\n",
                "os.makedirs(SCENE_DIR, exist_ok=True)\n",
                "os.makedirs(AUDIO_DIR, exist_ok=True)\n",
                "os.makedirs(FINAL_VIDEO_DIR, exist_ok=True)\n",
                "os.makedirs(SCRIPTS_DIR, exist_ok=True)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 0: Initialize Stable Diffusion Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 19,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Loading Stable Diffusion on cuda...\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Loading pipeline components...:  17%|‚ñà‚ñã        | 1/6 [00:00<00:00,  8.25it/s]\n"
                    ]
                },
                {
                    "ename": "ValueError",
                    "evalue": "The component <class 'transformers.models.clip.modeling_clip._LazyModule.__getattr__.<locals>.Placeholder'> of <class 'diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion.StableDiffusionPipeline'> cannot be loaded as it does not seem to have any of the loading methods defined in {'ModelMixin': ['save_pretrained', 'from_pretrained'], 'SchedulerMixin': ['save_pretrained', 'from_pretrained'], 'DiffusionPipeline': ['save_pretrained', 'from_pretrained'], 'OnnxRuntimeModel': ['save_pretrained', 'from_pretrained'], 'BaseGuidance': ['save_pretrained', 'from_pretrained'], 'PreTrainedTokenizer': ['save_pretrained', 'from_pretrained'], 'PreTrainedTokenizerFast': ['save_pretrained', 'from_pretrained'], 'PreTrainedModel': ['save_pretrained', 'from_pretrained'], 'FeatureExtractionMixin': ['save_pretrained', 'from_pretrained'], 'ProcessorMixin': ['save_pretrained', 'from_pretrained'], 'ImageProcessingMixin': ['save_pretrained', 'from_pretrained'], 'ORTModule': ['save_pretrained', 'from_pretrained']}.",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
                        "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
                        "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mLoading Stable Diffusion on \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdevice\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# ===== LOAD PIPELINE =====\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m pipe = \u001b[43mStableDiffusionPipeline\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfloat16\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcuda\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfloat32\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m    \u001b[49m\u001b[43msafety_checker\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n\u001b[32m     14\u001b[39m \u001b[43m)\u001b[49m.to(device)\n\u001b[32m     16\u001b[39m \u001b[38;5;66;03m# Enable memory optimizations\u001b[39;00m\n\u001b[32m     17\u001b[39m pipe.enable_attention_slicing()\n",
                        "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\navee\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\utils\\_validators.py:89\u001b[39m, in \u001b[36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     85\u001b[39m         validate_repo_id(arg_value)\n\u001b[32m     87\u001b[39m kwargs = smoothly_deprecate_legacy_arguments(fn_name=fn.\u001b[34m__name__\u001b[39m, kwargs=kwargs)\n\u001b[32m---> \u001b[39m\u001b[32m89\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
                        "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\navee\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\diffusers\\pipelines\\pipeline_utils.py:1021\u001b[39m, in \u001b[36mDiffusionPipeline.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[39m\n\u001b[32m   1014\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1015\u001b[39m     \u001b[38;5;66;03m# load sub model\u001b[39;00m\n\u001b[32m   1016\u001b[39m     sub_model_dtype = (\n\u001b[32m   1017\u001b[39m         torch_dtype.get(name, torch_dtype.get(\u001b[33m\"\u001b[39m\u001b[33mdefault\u001b[39m\u001b[33m\"\u001b[39m, torch.float32))\n\u001b[32m   1018\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(torch_dtype, \u001b[38;5;28mdict\u001b[39m)\n\u001b[32m   1019\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m torch_dtype\n\u001b[32m   1020\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1021\u001b[39m     loaded_sub_model = \u001b[43mload_sub_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1022\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlibrary_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlibrary_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1023\u001b[39m \u001b[43m        \u001b[49m\u001b[43mclass_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mclass_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1024\u001b[39m \u001b[43m        \u001b[49m\u001b[43mimportable_classes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mimportable_classes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1025\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpipelines\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpipelines\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1026\u001b[39m \u001b[43m        \u001b[49m\u001b[43mis_pipeline_module\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_pipeline_module\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1027\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpipeline_class\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpipeline_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1028\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43msub_model_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1029\u001b[39m \u001b[43m        \u001b[49m\u001b[43mprovider\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprovider\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1030\u001b[39m \u001b[43m        \u001b[49m\u001b[43msess_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43msess_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1031\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcurrent_device_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1032\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_memory\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_memory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1033\u001b[39m \u001b[43m        \u001b[49m\u001b[43moffload_folder\u001b[49m\u001b[43m=\u001b[49m\u001b[43moffload_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1034\u001b[39m \u001b[43m        \u001b[49m\u001b[43moffload_state_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43moffload_state_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1035\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel_variants\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_variants\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1036\u001b[39m \u001b[43m        \u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1037\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfrom_flax\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfrom_flax\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1038\u001b[39m \u001b[43m        \u001b[49m\u001b[43mvariant\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvariant\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1039\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1040\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcached_folder\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcached_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1041\u001b[39m \u001b[43m        \u001b[49m\u001b[43muse_safetensors\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_safetensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1042\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdduf_entries\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdduf_entries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1043\u001b[39m \u001b[43m        \u001b[49m\u001b[43mprovider_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprovider_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1044\u001b[39m \u001b[43m        \u001b[49m\u001b[43mquantization_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquantization_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1045\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1046\u001b[39m     logger.info(\n\u001b[32m   1047\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mLoaded \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m as \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclass_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m from `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m` subfolder of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained_model_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1048\u001b[39m     )\n\u001b[32m   1050\u001b[39m init_kwargs[name] = loaded_sub_model  \u001b[38;5;66;03m# UNet(...), # DiffusionSchedule(...)\u001b[39;00m\n",
                        "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\navee\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\diffusers\\pipelines\\pipeline_loading_utils.py:794\u001b[39m, in \u001b[36mload_sub_model\u001b[39m\u001b[34m(library_name, class_name, importable_classes, pipelines, is_pipeline_module, pipeline_class, torch_dtype, provider, sess_options, device_map, max_memory, offload_folder, offload_state_dict, model_variants, name, from_flax, variant, low_cpu_mem_usage, cached_folder, use_safetensors, dduf_entries, provider_options, quantization_config)\u001b[39m\n\u001b[32m    790\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m is_dummy_path \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mdummy\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m none_module:\n\u001b[32m    791\u001b[39m         \u001b[38;5;66;03m# call class_obj for nice error message of missing requirements\u001b[39;00m\n\u001b[32m    792\u001b[39m         class_obj()\n\u001b[32m--> \u001b[39m\u001b[32m794\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    795\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mThe component \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclass_obj\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpipeline_class\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m cannot be loaded as it does not seem to have\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    796\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m any of the loading methods defined in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mALL_IMPORTABLE_CLASSES\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    797\u001b[39m     )\n\u001b[32m    799\u001b[39m load_method = _get_load_method(class_obj, load_method_name, is_dduf=dduf_entries \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    801\u001b[39m \u001b[38;5;66;03m# add kwargs to loading method\u001b[39;00m\n",
                        "\u001b[31mValueError\u001b[39m: The component <class 'transformers.models.clip.modeling_clip._LazyModule.__getattr__.<locals>.Placeholder'> of <class 'diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion.StableDiffusionPipeline'> cannot be loaded as it does not seem to have any of the loading methods defined in {'ModelMixin': ['save_pretrained', 'from_pretrained'], 'SchedulerMixin': ['save_pretrained', 'from_pretrained'], 'DiffusionPipeline': ['save_pretrained', 'from_pretrained'], 'OnnxRuntimeModel': ['save_pretrained', 'from_pretrained'], 'BaseGuidance': ['save_pretrained', 'from_pretrained'], 'PreTrainedTokenizer': ['save_pretrained', 'from_pretrained'], 'PreTrainedTokenizerFast': ['save_pretrained', 'from_pretrained'], 'PreTrainedModel': ['save_pretrained', 'from_pretrained'], 'FeatureExtractionMixin': ['save_pretrained', 'from_pretrained'], 'ProcessorMixin': ['save_pretrained', 'from_pretrained'], 'ImageProcessingMixin': ['save_pretrained', 'from_pretrained'], 'ORTModule': ['save_pretrained', 'from_pretrained']}."
                    ]
                }
            ],
            "source": [
                "# ===== SETTINGS =====\n",
                "model_id = \"runwayml/stable-diffusion-v1-5\"\n",
                "lora_id = \"latent-consistency/lcm-lora-sdv1-5\"\n",
                "\n",
                "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
                "\n",
                "print(f\"Loading Stable Diffusion on {device}...\")\n",
                "\n",
                "# ===== LOAD PIPELINE =====\n",
                "pipe = StableDiffusionPipeline.from_pretrained(\n",
                "    model_id,\n",
                "    torch_dtype=torch.float16 if device == \"cuda\" else torch.float32,\n",
                "    safety_checker=None\n",
                ").to(device)\n",
                "\n",
                "# Enable memory optimizations\n",
                "pipe.enable_attention_slicing()\n",
                "pipe.enable_vae_slicing()\n",
                "if device == \"cuda\":\n",
                "    pipe.enable_model_cpu_offload()\n",
                "\n",
                "# ===== LOAD LCM LoRA =====\n",
                "pipe.load_lora_weights(lora_id)\n",
                "pipe.scheduler = LCMScheduler.from_config(pipe.scheduler.config)\n",
                "\n",
                "def generate_image_sd(prompt_text, output_path):\n",
                "    print(f\"Generating image for: {prompt_text}...\")\n",
                "    prompt = f\"\"\"\n",
                "    simple flat illustration of {prompt_text},\n",
                "    minimal design,\n",
                "    clean white background,\n",
                "    educational graphic,\n",
                "    vector style,\n",
                "    no text\n",
                "    \"\"\"\n",
                "    \n",
                "    try:\n",
                "        image = pipe(\n",
                "            prompt=prompt,\n",
                "            num_inference_steps=6,      # VERY LOW = FAST\n",
                "            guidance_scale=1.5,         # LCM works best low\n",
                "            height=512,\n",
                "            width=512\n",
                "        ).images[0]\n",
                "        \n",
                "        image.save(output_path)\n",
                "        return output_path\n",
                "    except Exception as e:\n",
                "        print(f\"Error generating image: {e}\")\n",
                "        return None"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 1: Script Generation with Ollama"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def generate_script(topic):\n",
                "    \"\"\"\n",
                "    Generate a fresh video script from Ollama based on the given topic.\n",
                "    Always creates NEW content - never uses cached/old scripts.\n",
                "    \"\"\"\n",
                "    prompt = f\"\"\"\n",
                "    Convert this topic into a structured video plan.\n",
                "    Topic: {topic}\n",
                "    Return JSON only:\n",
                "    {{\n",
                "      \"scenes\": [\n",
                "        {{\n",
                "          \"title\": \"\",\n",
                "          \"bullets\": [],\n",
                "          \"narration\": \"\",\n",
                "          \"image_prompt\": \"visual description for illustration\"\n",
                "        }}\n",
                "      ]\n",
                "    }}\n",
                "    \"\"\"\n",
                "    \n",
                "    print(f\"üé¨ Generating NEW script for: {topic}...\")\n",
                "    print(\"‚öôÔ∏è  Connecting to Ollama...\")\n",
                "    \n",
                "    # Generate fresh content from Ollama (REQUIRED)\n",
                "    try:\n",
                "        response = requests.post(OLLAMA_API_URL, json={\n",
                "            \"model\": OLLAMA_MODEL,\n",
                "            \"prompt\": prompt,\n",
                "            \"format\": \"json\",\n",
                "            \"stream\": False\n",
                "        }, timeout=300)\n",
                "        response.raise_for_status()\n",
                "        script_data = json.loads(response.json()['response'])\n",
                "        \n",
                "        # Create a safe filename from the topic\n",
                "        safe_topic = \"\".join(c if c.isalnum() or c in (' ', '-', '_') else '_' for c in topic)\n",
                "        safe_topic = safe_topic.replace(' ', '_')[:50]  # Limit length\n",
                "        \n",
                "        # Save the fresh plan to scripts directory with topic-based name\n",
                "        timestamp = __import__('datetime').datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
                "        plan_filename = f\"{safe_topic}_{timestamp}.json\"\n",
                "        plan_file = os.path.join(SCRIPTS_DIR, plan_filename)\n",
                "        \n",
                "        with open(plan_file, \"w\") as f:\n",
                "            json.dump(script_data, f, indent=2)\n",
                "        \n",
                "        print(f\"‚úÖ Fresh script generated and saved to: {plan_file}\")\n",
                "        return script_data\n",
                "        \n",
                "    except requests.exceptions.RequestException as e:\n",
                "        print(f\"‚ùå Cannot connect to Ollama: {e}\")\n",
                "        print(\"üìã Please ensure Ollama is running:\")\n",
                "        print(\"   1. Open terminal and run: ollama serve\")\n",
                "        print(\"   2. Or ensure Ollama service is active\")\n",
                "        return None\n",
                "        \n",
                "    except json.JSONDecodeError as e:\n",
                "        print(f\"‚ùå Error decoding JSON from Ollama: {e}\")\n",
                "        print(\"   The model may have returned invalid JSON format\")\n",
                "        return None"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 2: Slide Generation (PIL)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def create_slide(scene, index, image_path=None):\n",
                "    width, height = 1280, 720\n",
                "    img = Image.new('RGB', (width, height), color='white')\n",
                "    draw = ImageDraw.Draw(img)\n",
                "    \n",
                "    # Fonts\n",
                "    try:\n",
                "        title_font = ImageFont.truetype(\"arial.ttf\", 60)\n",
                "        text_font = ImageFont.truetype(\"arial.ttf\", 35)\n",
                "    except:\n",
                "        title_font = ImageFont.load_default()\n",
                "        text_font = ImageFont.load_default()\n",
                "    \n",
                "    # Layout Configuration\n",
                "    margin = 50\n",
                "    content_width = width - (2 * margin)\n",
                "    \n",
                "    # If image exists, we use split layout: Text (Left) | Image (Right)\n",
                "    if image_path and os.path.exists(image_path):\n",
                "        try:\n",
                "            sd_img = Image.open(image_path)\n",
                "            # Resize to fit right side but keep aspect ratio or simple fit\n",
                "            # Let's make it 512x512 centered on the right half, or scaled nicely\n",
                "            # Right half starts at x = 640\n",
                "            \n",
                "            # Target height 600, maintain aspect\n",
                "            target_ih = 600\n",
                "            aspect = sd_img.width / sd_img.height\n",
                "            target_iw = int(target_ih * aspect)\n",
                "            \n",
                "            sd_img = sd_img.resize((target_iw, target_ih), Image.Resampling.LANCZOS)\n",
                "            \n",
                "            # Position on right side\n",
                "            img_x = 640 + (640 - target_iw) // 2\n",
                "            img_y = (720 - target_ih) // 2\n",
                "            \n",
                "            img.paste(sd_img, (img_x, img_y))\n",
                "            \n",
                "            # Constrain text to left half\n",
                "            content_width = 580 # 640 - margin - padding\n",
                "        except Exception as e:\n",
                "            print(f\"Error placing image: {e}\")\n",
                "\n",
                "    # Draw Title\n",
                "    title_text = scene.get('title', f\"Scene {index}\")\n",
                "    # Wrap title if needed\n",
                "    title_lines = textwrap.wrap(title_text, width=20 if content_width < 600 else 40)\n",
                "    ty = 50\n",
                "    for line in title_lines:\n",
                "        draw.text((margin, ty), line, fill='black', font=title_font)\n",
                "        ty += 70\n",
                "    \n",
                "    # Draw Bullets\n",
                "    y = ty + 30\n",
                "    bullets = scene.get('bullets', [])\n",
                "    for bullet in bullets:\n",
                "        lines = textwrap.wrap(bullet, width=30 if content_width < 600 else 50)\n",
                "        for line in lines:\n",
                "            draw.text((margin + 30, y), f\"‚Ä¢ {line}\", fill='black', font=text_font)\n",
                "            y += 45\n",
                "            \n",
                "    filename = os.path.join(SCENE_DIR, f\"scene_{index}.png\")\n",
                "    img.save(filename)\n",
                "    return filename"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 3: Audio Generation (Edge-TTS)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "async def generate_audio(text, index):\n",
                "    voice = \"en-US-ChristopherNeural\"\n",
                "    output_file = os.path.join(AUDIO_DIR, f\"scene_{index}.mp3\")\n",
                "    \n",
                "    print(f\"Generating audio for scene {index}...\")\n",
                "    try:\n",
                "        communicate = edge_tts.Communicate(text, voice)\n",
                "        await communicate.save(output_file)\n",
                "        return output_file\n",
                "    except Exception as e:\n",
                "        print(f\"Error generating audio: {e}\")\n",
                "        return None"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 4: Video Assembly (MoviePy)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def create_video_clip(image_path, audio_path, index):\n",
                "    output_path = os.path.join(FINAL_VIDEO_DIR, f\"scene_{index}.mp4\")\n",
                "    \n",
                "    print(f\"Creating video clip for scene {index} using MoviePy...\")\n",
                "    try:\n",
                "        # Load audio first to get duration\n",
                "        audio_clip = AudioFileClip(audio_path)\n",
                "        \n",
                "        # Create video clip with proper FPS setting before adding audio\n",
                "        video_clip = (ImageClip(image_path)\n",
                "                     .with_duration(audio_clip.duration)\n",
                "                     .with_fps(24)  # Must set FPS before adding audio\n",
                "                     .with_audio(audio_clip))\n",
                "        \n",
                "        # Write with explicit audio settings for better compatibility\n",
                "        video_clip.write_videofile(\n",
                "            output_path, \n",
                "            fps=24, \n",
                "            codec='libx264',\n",
                "            audio_codec='aac',\n",
                "            audio_bitrate='192k',\n",
                "            preset='medium',\n",
                "            threads=4,\n",
                "            logger=None  # Suppress verbose output\n",
                "        )\n",
                "        \n",
                "        # Close clips to ensure file writes are complete\n",
                "        video_clip.close()\n",
                "        audio_clip.close()\n",
                "        \n",
                "        # Small delay to ensure file is fully written\n",
                "        import time\n",
                "        time.sleep(0.5)\n",
                "        \n",
                "        # Verify the output file has audio\n",
                "        test_clip = VideoFileClip(output_path)\n",
                "        has_audio = test_clip.audio is not None\n",
                "        test_clip.close()\n",
                "        \n",
                "        if has_audio:\n",
                "            print(f\"‚úì Audio verified in scene {index}\")\n",
                "        else:\n",
                "            print(f\"‚ö†Ô∏è  Warning: Audio missing from scene {index}\")\n",
                "        \n",
                "        return output_path\n",
                "    except Exception as e:\n",
                "        print(f\"MoviePy failed for scene {index}: {e}\")\n",
                "        import traceback\n",
                "        traceback.print_exc()\n",
                "        return None"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 5: Merge All Scenes"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def merge_scenes(video_files):\n",
                "    output_filename = os.path.join(FINAL_VIDEO_DIR, \"final_video.mp4\")\n",
                "    \n",
                "    print(\"Merging all scenes into final video...\")\n",
                "    try:\n",
                "        clips = []\n",
                "        for f in video_files:\n",
                "            print(f\"Loading {os.path.basename(f)}...\")\n",
                "            clip = VideoFileClip(f)\n",
                "            \n",
                "            # Verify audio is present\n",
                "            if clip.audio is None:\n",
                "                print(f\"‚ö†Ô∏è  Warning: No audio in {f}\")\n",
                "            else:\n",
                "                print(f\"‚úì Audio loaded: {clip.audio.duration:.2f}s\")\n",
                "            \n",
                "            clips.append(clip)\n",
                "        \n",
                "        print(f\"\\nConcatenating {len(clips)} clips...\")\n",
                "        # Use default method (chain) which preserves audio better\n",
                "        final_clip = concatenate_videoclips(clips)\n",
                "        \n",
                "        print(\"Writing final video with audio...\")\n",
                "        final_clip.write_videofile(\n",
                "            output_filename, \n",
                "            fps=24, \n",
                "            codec='libx264',\n",
                "            audio_codec='aac',\n",
                "            audio_bitrate='192k',\n",
                "            preset='medium',\n",
                "            threads=4,\n",
                "            logger=None  # Suppress verbose output\n",
                "        )\n",
                "        \n",
                "        # Close all clips to free resources\n",
                "        final_clip.close()\n",
                "        for clip in clips:\n",
                "            clip.close()\n",
                "        \n",
                "        print(f\"‚úÖ Done! Output: {output_filename}\")\n",
                "        return output_filename\n",
                "    except Exception as e:\n",
                "        print(f\"‚ùå Error merging scenes: {e}\")\n",
                "        import traceback\n",
                "        traceback.print_exc()\n",
                "        return None"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Execution Pipeline"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "async def main(topic):\n",
                "    \"\"\"\n",
                "    Main video generation pipeline.\n",
                "    Always generates fresh content based on the given topic.\n",
                "    \"\"\"\n",
                "    # 1. Generate Script (FRESH - never uses cache)\n",
                "    script_data = generate_script(topic)\n",
                "    if not script_data:\n",
                "        print(\"‚ùå Cannot proceed without a valid script. Exiting...\")\n",
                "        return\n",
                "    \n",
                "    scenes = script_data.get('scenes', [])\n",
                "    if not scenes:\n",
                "        print(\"‚ùå No scenes found in script. Exiting...\")\n",
                "        return\n",
                "    \n",
                "    video_clips = []\n",
                "    \n",
                "    print(f\"\\nüé• Starting video generation for {len(scenes)} scenes...\\n\")\n",
                "    \n",
                "    for i, scene in enumerate(scenes, 1):\n",
                "        title = scene.get('title')\n",
                "        print(f\"üìç Processing Scene {i}/{len(scenes)}: {title}\")\n",
                "        \n",
                "        # 1.5 Generate Image (SD)\n",
                "        image_prompt = scene.get('image_prompt')\n",
                "        generated_img_path = None\n",
                "        if image_prompt:\n",
                "             # Create a safe filename for the raw SD generation\n",
                "             raw_img_path = os.path.join(SCENE_DIR, f\"scene_{i}_raw.png\")\n",
                "             generated_img_path = generate_image_sd(image_prompt, raw_img_path)\n",
                "        \n",
                "        if not generated_img_path:\n",
                "             # Fallback if generation failed or no prompt, create_slide handles None\n",
                "             print(\"‚ö†Ô∏è  No image generated, using text-only layout.\")\n",
                "\n",
                "        # 2. Create Slide with Text + Image\n",
                "        img_path = create_slide(scene, i, image_path=generated_img_path)\n",
                "        \n",
                "        # 3. Narration -> Audio\n",
                "        narration = scene.get('narration', '')\n",
                "        if not narration:\n",
                "            print(f\"‚ö†Ô∏è  Warning: No narration for scene {i}, skipping...\")\n",
                "            continue\n",
                "            \n",
                "        audio_path = await generate_audio(narration, i)\n",
                "        if not audio_path:\n",
                "            continue\n",
                "            \n",
                "        # 4. Combine -> Video Clip\n",
                "        clip_path = create_video_clip(img_path, audio_path, i)\n",
                "        if clip_path:\n",
                "            video_clips.append(clip_path)\n",
                "            print(f\"‚úÖ Scene {i} completed\\n\")\n",
                "            \n",
                "    # 5. Merge All Scenes\n",
                "    if video_clips:\n",
                "        print(f\"\\nüé¨ Merging {len(video_clips)} scenes into final video...\")\n",
                "        final_path = merge_scenes(video_clips)\n",
                "        if final_path:\n",
                "            print(f\"\\nüéâ SUCCESS! Your video is ready: {final_path}\")\n",
                "    else:\n",
                "        print(\"‚ùå No video clips were created. Cannot generate final video.\")\n",
                "\n",
                "# Example Usage"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "await main(\"Model context protocol\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.9"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
