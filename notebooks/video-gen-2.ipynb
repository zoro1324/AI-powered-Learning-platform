{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Video Generation Pipeline\n",
                "\n",
                "This notebook generates a video from a given topic or text using Ollama for scripting, Stable Diffusion (Diffusers) for images, Edge-TTS for audio, and MoviePy for assembly."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import json\n",
                "import os\n",
                "import textwrap\n",
                "import requests\n",
                "import asyncio\n",
                "from PIL import Image, ImageDraw, ImageFont\n",
                "import edge_tts\n",
                "from moviepy import ImageClip, AudioFileClip, concatenate_videoclips, VideoFileClip\n",
                "import torch\n",
                "from diffusers import StableDiffusionPipeline, LCMScheduler\n",
                "from peft import get_peft_model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "print(torch.__version__)\n",
                "print(torch.version.cuda)\n",
                "print(torch.cuda.is_available())\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Configuration\n",
                "OLLAMA_API_URL = \"http://localhost:11434/api/generate\"\n",
                "# Use 'llama3' or another model you have installed\n",
                "OLLAMA_MODEL = \"phi3:mini\"\n",
                "\n",
                "OUTPUT_DIR = \"output\"\n",
                "SCENE_DIR = os.path.join(OUTPUT_DIR, \"scenes\")\n",
                "AUDIO_DIR = os.path.join(OUTPUT_DIR, \"audio\")\n",
                "FINAL_VIDEO_DIR = os.path.join(OUTPUT_DIR, \"video\")\n",
                "\n",
                "os.makedirs(SCENE_DIR, exist_ok=True)\n",
                "os.makedirs(AUDIO_DIR, exist_ok=True)\n",
                "os.makedirs(FINAL_VIDEO_DIR, exist_ok=True)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 0: Initialize Stable Diffusion Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ===== SETTINGS =====\n",
                "model_id = \"runwayml/stable-diffusion-v1-5\"\n",
                "lora_id = \"latent-consistency/lcm-lora-sdv1-5\"\n",
                "\n",
                "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
                "\n",
                "print(f\"Loading Stable Diffusion on {device}...\")\n",
                "\n",
                "# ===== LOAD PIPELINE =====\n",
                "pipe = StableDiffusionPipeline.from_pretrained(\n",
                "    model_id,\n",
                "    torch_dtype=torch.float16 if device == \"cuda\" else torch.float32,\n",
                "    safety_checker=None\n",
                ").to(device)\n",
                "\n",
                "# Enable memory optimizations\n",
                "pipe.enable_attention_slicing()\n",
                "pipe.enable_vae_slicing()\n",
                "if device == \"cuda\":\n",
                "    pipe.enable_model_cpu_offload()\n",
                "\n",
                "# ===== LOAD LCM LoRA =====\n",
                "pipe.load_lora_weights(lora_id)\n",
                "pipe.scheduler = LCMScheduler.from_config(pipe.scheduler.config)\n",
                "\n",
                "def generate_image_sd(prompt_text, output_path):\n",
                "    print(f\"Generating image for: {prompt_text}...\")\n",
                "    prompt = f\"\"\"\n",
                "    simple flat illustration of {prompt_text},\n",
                "    minimal design,\n",
                "    clean white background,\n",
                "    educational graphic,\n",
                "    vector style,\n",
                "    no text\n",
                "    \"\"\"\n",
                "    \n",
                "    try:\n",
                "        image = pipe(\n",
                "            prompt=prompt,\n",
                "            num_inference_steps=10,      # VERY LOW = FAST\n",
                "            guidance_scale=1.5,         # LCM works best low\n",
                "            height=512,\n",
                "            width=512\n",
                "        ).images[0]\n",
                "        \n",
                "        image.save(output_path)\n",
                "        return output_path\n",
                "    except Exception as e:\n",
                "        print(f\"Error generating image: {e}\")\n",
                "        return None"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 1: Script Generation with Ollama"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def generate_script(topic):\n",
                "    prompt = f\"\"\"\n",
                "    Convert this topic into a structured video plan.\n",
                "    Topic: {topic}\n",
                "    Return JSON only:\n",
                "    {{\n",
                "      \"scenes\": [\n",
                "        {{\n",
                "          \"title\": \"\",\n",
                "          \"bullets\": [],\n",
                "          \"narration\": \"\",\n",
                "          \"image_prompt\": \"visual description for illustration\"\n",
                "        }}\n",
                "      ]\n",
                "    }}\n",
                "    \"\"\"\n",
                "    \n",
                "    print(f\"Generating script for: {topic}...\")\n",
                "    try:\n",
                "        response = requests.post(OLLAMA_API_URL, json={\n",
                "            \"model\": OLLAMA_MODEL,\n",
                "            \"prompt\": prompt,\n",
                "            \"format\": \"json\",\n",
                "            \"stream\": False\n",
                "        })\n",
                "        response.raise_for_status()\n",
                "        return json.loads(response.json()['response'])\n",
                "    except requests.exceptions.RequestException as e:\n",
                "        print(f\"Error connecting to Ollama: {e}\")\n",
                "        return None\n",
                "    except json.JSONDecodeError:\n",
                "        print(\"Error decoding JSON response from Ollama.\")\n",
                "        return None"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 2: Slide Generation (PIL)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def create_slide(scene, index, image_path=None):\n",
                "    width, height = 1280, 720\n",
                "    img = Image.new('RGB', (width, height), color='white')\n",
                "    draw = ImageDraw.Draw(img)\n",
                "    \n",
                "    # Fonts\n",
                "    try:\n",
                "        title_font = ImageFont.truetype(\"arial.ttf\", 60)\n",
                "        text_font = ImageFont.truetype(\"arial.ttf\", 35)\n",
                "    except:\n",
                "        title_font = ImageFont.load_default()\n",
                "        text_font = ImageFont.load_default()\n",
                "    \n",
                "    # Layout Configuration\n",
                "    margin = 50\n",
                "    content_width = width - (2 * margin)\n",
                "    \n",
                "    # If image exists, we use split layout: Text (Left) | Image (Right)\n",
                "    if image_path and os.path.exists(image_path):\n",
                "        try:\n",
                "            sd_img = Image.open(image_path)\n",
                "            # Resize to fit right side but keep aspect ratio or simple fit\n",
                "            # Let's make it 512x512 centered on the right half, or scaled nicely\n",
                "            # Right half starts at x = 640\n",
                "            \n",
                "            # Target height 600, maintain aspect\n",
                "            target_ih = 600\n",
                "            aspect = sd_img.width / sd_img.height\n",
                "            target_iw = int(target_ih * aspect)\n",
                "            \n",
                "            sd_img = sd_img.resize((target_iw, target_ih), Image.Resampling.LANCZOS)\n",
                "            \n",
                "            # Position on right side\n",
                "            img_x = 640 + (640 - target_iw) // 2\n",
                "            img_y = (720 - target_ih) // 2\n",
                "            \n",
                "            img.paste(sd_img, (img_x, img_y))\n",
                "            \n",
                "            # Constrain text to left half\n",
                "            content_width = 580 # 640 - margin - padding\n",
                "        except Exception as e:\n",
                "            print(f\"Error placing image: {e}\")\n",
                "\n",
                "    # Draw Title\n",
                "    title_text = scene.get('title', f\"Scene {index}\")\n",
                "    # Wrap title if needed\n",
                "    title_lines = textwrap.wrap(title_text, width=20 if content_width < 600 else 40)\n",
                "    ty = 50\n",
                "    for line in title_lines:\n",
                "        draw.text((margin, ty), line, fill='black', font=title_font)\n",
                "        ty += 70\n",
                "    \n",
                "    # Draw Bullets\n",
                "    y = ty + 30\n",
                "    bullets = scene.get('bullets', [])\n",
                "    for bullet in bullets:\n",
                "        lines = textwrap.wrap(bullet, width=30 if content_width < 600 else 50)\n",
                "        for line in lines:\n",
                "            draw.text((margin + 30, y), f\"â€¢ {line}\", fill='black', font=text_font)\n",
                "            y += 45\n",
                "            \n",
                "    filename = os.path.join(SCENE_DIR, f\"scene_{index}.png\")\n",
                "    img.save(filename)\n",
                "    return filename"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 3: Audio Generation (Edge-TTS)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "async def generate_audio(text, index):\n",
                "    voice = \"en-US-ChristopherNeural\"\n",
                "    output_file = os.path.join(AUDIO_DIR, f\"scene_{index}.mp3\")\n",
                "    \n",
                "    print(f\"Generating audio for scene {index}...\")\n",
                "    try:\n",
                "        communicate = edge_tts.Communicate(text, voice)\n",
                "        await communicate.save(output_file)\n",
                "        return output_file\n",
                "    except Exception as e:\n",
                "        print(f\"Error generating audio: {e}\")\n",
                "        return None"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 4: Video Assembly (MoviePy)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def create_video_clip(image_path, audio_path, index):\n",
                "    output_path = os.path.join(FINAL_VIDEO_DIR, f\"scene_{index}.mp4\")\n",
                "    \n",
                "    print(f\"Creating video clip for scene {index} using MoviePy...\")\n",
                "    try:\n",
                "        audio_clip = AudioFileClip(audio_path)\n",
                "        video_clip = ImageClip(image_path).with_duration(audio_clip.duration)\n",
                "        video_clip = video_clip.with_audio(audio_clip)\n",
                "        video_clip.write_videofile(output_path, fps=24, codec='libx264', audio_codec='aac')\n",
                "        return output_path\n",
                "    except Exception as e:\n",
                "        print(f\"MoviePy failed for scene {index}: {e}\")\n",
                "        return None"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 5: Merge All Scenes"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def merge_scenes(video_files):\n",
                "    output_filename = \"final_video.mp4\"\n",
                "    \n",
                "    print(\"Merging all scenes into final video...\")\n",
                "    try:\n",
                "        clips = [VideoFileClip(f) for f in video_files]\n",
                "        final_clip = concatenate_videoclips(clips)\n",
                "        final_clip.write_videofile(output_filename, fps=24, codec='libx264', audio_codec='aac')\n",
                "        print(f\"Done! Output: {output_filename}\")\n",
                "        return output_filename\n",
                "    except Exception as e:\n",
                "        print(f\"Error merging scenes: {e}\")\n",
                "        return None"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Execution Pipeline"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "async def main(topic):\n",
                "    # 1. Generate Script\n",
                "    script_data = generate_script(topic)\n",
                "    if not script_data:\n",
                "        return\n",
                "    \n",
                "    # Save plan for reference\n",
                "    with open(\"video_plan.json\", \"w\") as f:\n",
                "        json.dump(script_data, f, indent=2)\n",
                "    \n",
                "    scenes = script_data.get('scenes', [])\n",
                "    video_clips = []\n",
                "    \n",
                "    for i, scene in enumerate(scenes, 1):\n",
                "        title = scene.get('title')\n",
                "        print(f\"Processing Scene {i}: {title}\")\n",
                "        \n",
                "        # 1.5 Generate Image (SD)\n",
                "        image_prompt = scene.get('image_prompt')\n",
                "        generated_img_path = None\n",
                "        if image_prompt:\n",
                "             # Create a safe filename for the raw SD generation\n",
                "             raw_img_path = os.path.join(SCENE_DIR, f\"scene_{i}_raw.png\")\n",
                "             generated_img_path = generate_image_sd(image_prompt, raw_img_path)\n",
                "        \n",
                "        if not generated_img_path:\n",
                "             # Fallback if generation failed or no prompt, create_slide handles None\n",
                "             print(\"No image generated, using text-only layout.\")\n",
                "\n",
                "        # 2. Tools -> Image (Slide)\n",
                "        img_path = create_slide(scene, i, image_path=generated_img_path)\n",
                "        \n",
                "        # 3. Narration -> Audio\n",
                "        narration = scene.get('narration', '')\n",
                "        if not narration:\n",
                "            print(f\"Warning: No narration for scene {i}\")\n",
                "            continue\n",
                "            \n",
                "        audio_path = await generate_audio(narration, i)\n",
                "        if not audio_path:\n",
                "            continue\n",
                "            \n",
                "        # 4. Combine -> Clip\n",
                "        clip_path = create_video_clip(img_path, audio_path, i)\n",
                "        if clip_path:\n",
                "            video_clips.append(clip_path)\n",
                "            \n",
                "    # 5. Merge\n",
                "    if video_clips:\n",
                "        merge_scenes(video_clips)\n",
                "    else:\n",
                "        print(\"No video clips were created.\")\n",
                "\n",
                "# Example Usage\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "await main(\"The history of the Chess\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.9"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
