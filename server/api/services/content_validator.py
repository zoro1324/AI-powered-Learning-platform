"""
AI Content Validator
====================
A reviewer AI agent that validates all AI-generated content before it is
stored or returned to the user.

Supported content types
-----------------------
- "video_script"   — JSON script generated by VideoGeneratorService
- "podcast_script" — JSON conversation generated by PodcastService
- "notes"          — Markdown lesson content from GenerateTopicContentView
- "remediation"    — Markdown remediation notes from GenerateRemediationContentView

Usage
-----
    from api.services.content_validator import validate_content, ValidationResult

    result = validate_content(
        content="<generated text>",
        topic="Introduction to Neural Networks",
        content_type="notes",
        user_requirements="Explain using real-world examples",
        max_retries=5,           # default: 5
    )

    if result.approved:
        # use the content
    else:
        logger.warning("Rejected: %s", result.issues)
"""

import json
import logging
from dataclasses import dataclass, field
from typing import List

from api.services.ai_client import generate_text

logger = logging.getLogger(__name__)


# ─────────────────────────────────────────────────────────────────────────────
# Data model
# ─────────────────────────────────────────────────────────────────────────────

@dataclass
class ValidationResult:
    """Result returned by the reviewer AI agent."""
    approved: bool
    feedback: str = ""
    issues: List[str] = field(default_factory=list)


# ─────────────────────────────────────────────────────────────────────────────
# Internal helpers
# ─────────────────────────────────────────────────────────────────────────────

# Human-readable labels used in reviewer prompts
_CONTENT_TYPE_LABELS = {
    "video_script":    "educational video script (JSON)",
    "podcast_script":  "educational podcast script / conversation (JSON)",
    "notes":           "educational lesson notes (Markdown)",
    "remediation":     "remediation / corrective lesson notes (Markdown)",
}


def _build_reviewer_system_prompt(
    content_type: str,
    topic: str,
    user_requirements: str,
) -> str:
    """
    Build a detailed system prompt for the reviewer AI.

    The prompt instructs the reviewer to evaluate the content and
    respond **only** with a strict JSON object.
    """
    label = _CONTENT_TYPE_LABELS.get(content_type, "educational content")
    requirements_section = (
        f"\n\nUser-Specific Requirements / Learning Style:\n{user_requirements}"
        if user_requirements.strip()
        else ""
    )

    return f"""You are a strict educational content reviewer AI.

Your job is to evaluate {label} generated about the topic: "{topic}".{requirements_section}

Evaluation Criteria:
1. RELEVANCE     — The content must be clearly and specifically about "{topic}".
2. QUALITY       — It must be factually coherent, educationally valuable, well-structured, and complete.
3. COMPLETENESS  — It must not be empty, truncated, or consist only of placeholder text.
4. SAFETY        — It must be free of harmful, inappropriate, or offensive material.
5. USER FIT      — If user requirements are provided, the content should match the requested style/depth.

You MUST respond with a single, valid JSON object and nothing else.

Response format:
{{
  "approved": true | false,
  "feedback": "Brief explanation of your decision (1-3 sentences).",
  "issues": ["Issue 1", "Issue 2"]   // empty list if approved
}}
"""


def _parse_reviewer_response(raw: str) -> ValidationResult:
    """
    Parse the reviewer model's JSON response into a ValidationResult.
    Falls back to a 'rejected' result if the JSON is malformed.
    """
    try:
        # Strip markdown fences if the model wrapped the JSON
        text = raw.strip()
        if text.startswith("```"):
            lines = text.splitlines()
            # Remove first and last fence lines
            text = "\n".join(
                line for line in lines
                if not line.strip().startswith("```")
            ).strip()

        data = json.loads(text)
        return ValidationResult(
            approved=bool(data.get("approved", False)),
            feedback=str(data.get("feedback", "")),
            issues=list(data.get("issues", [])),
        )
    except (json.JSONDecodeError, TypeError, AttributeError) as exc:
        print(f"  [ContentValidator] ✗ JSON Parse Error: {exc}")
        print(f"  [ContentValidator] Raw output was: {raw[:500]}...")
        logger.warning(
            "[ContentValidator] Could not parse reviewer response: %s — raw=%r",
            exc, raw[:300],
        )
        return ValidationResult(
            approved=False,
            feedback="Reviewer returned unparseable output; treating as rejected.",
            issues=["Reviewer response was not valid JSON."],
        )


# ─────────────────────────────────────────────────────────────────────────────
# Public API
# ─────────────────────────────────────────────────────────────────────────────

def validate_content(
    content: str,
    topic: str,
    content_type: str,
    user_requirements: str = "",
    max_retries: int = 5,
) -> ValidationResult:
    """
    Send *content* to the reviewer AI and return a ValidationResult.

    This function performs a single review call.  Retry logic (regeneration
    + re-validation) lives in the callers (services / views).

    Args:
        content:           The AI-generated content to review.
        topic:             The topic/subject the content should be about.
        content_type:      One of "video_script", "podcast_script", "notes", "remediation".
        user_requirements: Any additional user context (learning style, scenario, etc.).
        max_retries:       Informational — logged for traceability; not used internally.

    Returns:
        ValidationResult with approved flag, feedback, and issues list.
    """
    if not content or not content.strip():
        logger.warning("[ContentValidator] Empty content submitted for review.")
        return ValidationResult(
            approved=False,
            feedback="Content is empty.",
            issues=["Generated content was empty or whitespace-only."],
        )

    print(f"\n  --- ContentValidator Review Start ---")
    print(f"  Type: {content_type}")
    print(f"  Topic: {topic}")
    
    system_prompt = _build_reviewer_system_prompt(content_type, topic, user_requirements)

    review_prompt = (
        f"Please review the following {_CONTENT_TYPE_LABELS.get(content_type, 'content')} "
        f"and respond with ONLY the JSON object described in your instructions.\n\n"
        f"--- CONTENT START ---\n{content[:6000]}\n--- CONTENT END ---"
    )

    logger.info(
        "[ContentValidator] Reviewing %s for topic=%r (max_retries=%d) ...",
        content_type, topic, max_retries,
    )

    try:
        raw_response = generate_text(
            review_prompt,
            system_prompt=system_prompt,
            json_mode=True,
        )
        print(f"  [ContentValidator] Raw Reviewer Response: {raw_response[:200]}...")
        result = _parse_reviewer_response(raw_response)

        if result.approved:
            print(f"  [ContentValidator] ✅ APPROVED: {result.feedback}")
            logger.info(
                "[ContentValidator] ✅ Approved | feedback=%r",
                result.feedback,
            )
        else:
            print(f"  [ContentValidator] ❌ REJECTED: {result.feedback}")
            print(f"  [ContentValidator] Issues: {result.issues}")
            logger.warning(
                "[ContentValidator] ❌ Rejected | feedback=%r | issues=%s",
                result.feedback,
                result.issues,
            )
        print(f"  --- ContentValidator Review End ---\n")
        return result

    except Exception as exc:
        print(f"  [ContentValidator] ✗ Critical Failure: {exc}")
        logger.exception("[ContentValidator] Reviewer call failed: %s", exc)
        # On reviewer failure, approve with a warning so pipeline can continue
        return ValidationResult(
            approved=True,
            feedback="Reviewer call failed; content passed through with warning.",
            issues=[],
        )


def validate_with_retry(
    generate_fn,
    topic: str,
    content_type: str,
    user_requirements: str = "",
    max_retries: int = 5,
):
    """
    Helper that ties generation + validation into a retry loop.

    Args:
        generate_fn:       A zero-argument callable that produces the content string.
                           On each attempt it will be called fresh.
        topic:             Topic to validate against.
        content_type:      One of the recognised content type strings.
        user_requirements: Extra context forwarded to the reviewer.
        max_retries:       Maximum number of generation+review cycles. Default 5.

    Returns:
        Tuple[str, ValidationResult] — the (possibly best-effort) content and
        the final ValidationResult.

    Raises:
        RuntimeError: if generate_fn raises on every attempt.
    """
    last_content = ""
    last_result = ValidationResult(approved=False, feedback="", issues=[])

    for attempt in range(1, max_retries + 1):
        logger.info(
            "[ContentValidator] Generation attempt %d/%d for %s topic=%r",
            attempt, max_retries, content_type, topic,
        )

        last_content = generate_fn()
        last_result = validate_content(
            last_content,
            topic=topic,
            content_type=content_type,
            user_requirements=user_requirements,
            max_retries=max_retries,
        )

        if last_result.approved:
            print(f"  [ContentValidator] Attempt {attempt} succeeded.")
            logger.info(
                "[ContentValidator] ✅ Approved on attempt %d/%d",
                attempt, max_retries,
            )
            return last_content, last_result

        print(f"  [ContentValidator] Attempt {attempt} REJECTED. Issues: {last_result.issues}")
        logger.warning(
            "[ContentValidator] ❌ Rejected (attempt %d/%d) | issues=%s",
            attempt, max_retries, last_result.issues,
        )
        if attempt < max_retries:
            print(f"  [ContentValidator] Retrying (Attempt {attempt + 1})...")
            logger.info("[ContentValidator] Retrying generation...")

    # All attempts exhausted — return the last content as best-effort
    logger.error(
        "[ContentValidator] All %d attempts rejected for %s topic=%r. "
        "Using best-effort content.",
        max_retries, content_type, topic,
    )
    return last_content, last_result
